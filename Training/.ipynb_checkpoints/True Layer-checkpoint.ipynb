{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fec7e277",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-a8ef42409438>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mio\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import io\n",
    "import pandas as pd\n",
    "import json\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Embedding, Dense, Conv1D, MaxPooling1D, Flatten\n",
    "from sklearn import preprocessing\n",
    "import pickle\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df52629",
   "metadata": {},
   "source": [
    "### Import Metadata CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95173eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pd.read_csv(os.getcwd() + '\\movies_metadata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb4ce4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213df994",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = pd.DataFrame()\n",
    "input_data['id'] = metadata['id'].copy()\n",
    "input_data['title'] = metadata['title'].copy()\n",
    "input_data['genres'] = metadata['genres'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc4b3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data['genre'] = ''\n",
    "input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ef2ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Just take first word\n",
    "for i in range(input_data.shape[0]):\n",
    "    \n",
    "    json_str = input_data.iloc[i,2]\n",
    "    json_str = json_str.strip('[]')\n",
    "    json_str = json_str.replace('{','')\n",
    "    json_str = json_str.replace('}','')\n",
    "    json_list = json_str.split(',')\n",
    "    for j in json_list[1:3:2]:\n",
    "        word = j.split(':')[1].strip(' ').strip('\\'')\n",
    "    \n",
    "    #words = word.strip(' ')\n",
    "    input_data.iloc[i,3] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616325f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "del input_data['genres']\n",
    "input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62f1093",
   "metadata": {},
   "source": [
    "### Import Keywords CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a66ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = pd.read_csv(os.getcwd() + '\\keywords.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc94344d",
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205c21c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbc4187",
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords['keywords_string'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1581c5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ba635a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(keywords.shape[0]):\n",
    "    #for i in range(1):\n",
    "    words = ''\n",
    "    json_str = keywords.iloc[i,1]\n",
    "    json_str = json_str.strip('[]')\n",
    "    json_str = json_str.replace('{','')\n",
    "    json_str = json_str.replace('}','')\n",
    "    json_list = json_str.split(',')\n",
    "\n",
    "    for j in json_list[1::2]:\n",
    "        word = j.split(':')[1].strip(' ').strip('\\'')\n",
    "        words += word + ' '\n",
    "    \n",
    "    words = words.strip(' ')\n",
    "    keywords.iloc[i,2] = words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a10c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "del keywords['keywords']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7d3f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9b8ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be35197",
   "metadata": {},
   "source": [
    "### Concat Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74501f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords['id'] = keywords['id'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d85f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44b4815",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_data = pd.merge(input_data, keywords, on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926f916b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_value = float(\"NaN\")\n",
    "movie_data.replace(\"\", nan_value, inplace=True)\n",
    "movie_data.dropna(subset = [\"title\", \"genre\", \"keywords_string\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4cc0c9b",
   "metadata": {},
   "source": [
    "### Create Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159a7116",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_data, test_movie_data = movie_data[:30656,:], movie_data[30656:,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8169ee12",
   "metadata": {},
   "source": [
    "### Create Input Strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc4ca72",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_data['input'] = movie_data['title'] + ' keywords ' + movie_data['keywords_string']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43829de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_data.drop(labels=['title', 'keywords_string', 'id'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772bb64d",
   "metadata": {},
   "source": [
    "### Create Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f716e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_data = movie_data.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5b18be",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_data, movie_data_test = movie_data.iloc[:30656,:], movie_data.iloc[30656:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ada3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_data_test.to_csv('movie_data_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca89b34b",
   "metadata": {},
   "source": [
    " ### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b3d9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844f9d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = [lemmatizer.lemmatize(token) for token in text.split(\" \")]\n",
    "    text = [word for word in text if  not word in stop_words]\n",
    "    text = \" \".join(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221fa1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_data['Processed_input'] = movie_data.input.apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7726d061",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = movie_data['Processed_input']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62bd542",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = movie_data['genre']\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(movie_data['genre'])\n",
    "Y = le.transform(movie_data['genre'])\n",
    "max(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57db9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Label_encoder.pickle', 'wb') as handle:\n",
    "    pickle.dump(le, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c04b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(movie_data['Processed_input'])\n",
    "encoded_docs = tokenizer.texts_to_sequences(movie_data['Processed_input'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f255c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving\n",
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb75e834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad sequences\n",
    "max_length = max([len(s.split()) for s in movie_data['Processed_input']])\n",
    "X = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "print(max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abc76b7",
   "metadata": {},
   "source": [
    "### Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3eb7265",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val = X[:28656,:], X[28656:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d75ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train, Y_val = Y[:28656], Y[28656:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d360e894",
   "metadata": {},
   "source": [
    "### Build Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66b33c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dict = {}\n",
    "with open(\"glove.6B.200d.txt\", 'r') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], \"float32\")\n",
    "        embeddings_dict[word] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1191f5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c38ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((vocab_size, 200))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_dict.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864e438a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(layers.Embedding(input_dim=vocab_size, \n",
    "                           output_dim=200,\n",
    "                           weights = [embedding_matrix],\n",
    "                           input_length=max_length,\n",
    "                           trainable=True))\n",
    "#model.add(layers.Conv1D(filters=256, kernel_size=3, activation='relu'))\n",
    "#model.add(layers.Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(20, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67a9634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile network\n",
    "model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(), optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78035a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b22c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model with early stopping and save best model\n",
    "past_accuracy = 0\n",
    "session = 0\n",
    "while True:\n",
    "    model.fit(X_train, Y_train, batch_size=128, epochs=1, verbose=2)\n",
    "    loss, accuracy = model.evaluate(X_val, Y_val)\n",
    "    if past_accuracy < accuracy:\n",
    "        past_accuracy = accuracy\n",
    "        model_save = model\n",
    "    else:\n",
    "        session += 1\n",
    "        if session == 3:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7363c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save.save(os.getcwd() + '\\movie_genre_model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
